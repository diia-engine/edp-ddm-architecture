= Deploying the Platform in a public _AWS_ cloud environment
include::platform:ROOT:partial$templates/document-attributes/default-set-en.adoc[]

include::platform:ROOT:partial$admonitions/language-en.adoc[]

This guide provides detailed instructions on deploying the Platform in an Amazon Web Services (AWS) environment from scratch, from creating an AWS account to installing and configuring the Platform.

== Prerequisites

Before installing and configuring the Platform, you must perform the following steps.

=== Elements required at the pre-deployment stage

Before you begin, make sure you have the resources that are required for further steps:

Documentation: ::

* [*] xref:release-notes:release-notes.adoc[Release notes];
* [*] xref:release-notes:backward-incompatible-changes.adoc[Backward incompatible changes];
* [*] xref:admin:update/overview.adoc[] document with additional steps for the selected Platform release version. It is required only for the Platform update procedure.

Digital signature certificates (digital-signature-ops certificates): ::

* [*] *_Key-6.dat_* -- your organization's private key;
* [*] *_allowed-key.yaml_* -- a list of all issued keys. Initially includes only _Key-6.dat_. When the key is changed, information about the new key is added here without deleting the old one;
* [*] *_CAs.json_* -- a list of all Accredited Key Certification Centers from the https://iit.com.ua/downloads[ІІТ] website;
* [*] *_CACertificates.p7b_* - a public key from the https://iit.com.ua/downloads[ІІТ] website.

Configuration files for the hardware cryptomodule: ::

* 3 files with appropriate values (_see attached examples_):

* [*] _link:{attachmentsdir}/aws-deployment/sign.key.device-type[sign.key.device-type]_ -- specify the device type for the key (file);
* [*] _link:{attachmentsdir}/aws-deployment/sign.key.file.issuer[ sign.key.file.issuer]_ -- specify the Accredited Key Certification Center that issued the key (change the value inside the file to match your issuer);
* [*] _link:{attachmentsdir}/aws-deployment/sign.key.file.password[sign.key.file.password]_ -- specify the password for the file key (change the value inside the file to match your password).
+
4 files with empty values (_create 4 empty files with the following names_):

* [*] *_sign.key.hardware.device_* -- key device type (hardware);
* [*] *_sign.key.hardware.password_* --  hardware key password;
* [*] *_sign.key.hardware.type_* -- key type;
* [*] *_osplm.ini_* -- INI configuration.

+
TIP: To learn more about loading and updating the keys and digital signature certificates, see xref:registry-management/system-keys/control-plane-platform-keys.adoc[].

* [*] a Docker image of the *`openshift-install`* container (_for details, see xref:#launch-openshift-install[]_);
* [*] a downloaded Installer -- a script for Platform deployment (_for details, see xref:#installer-preparation-launch[]_).

=== Creating an AWS account


Before installing OpenShift Container Platform on AWS, you need to create an AWS account.

To learn how to do this, refer to the AWS documentation: https://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account/[How do I create and activate a new AWS account?]

=== Setting up an AWS account

Before installing OpenShift Container Platform, you need to set up your AWS account.

[#setup-route-53]
==== Configuring Route 53

To install OpenShift Container Platform, you need to register a domain name. You can do this using the *Amazon Route 53* service or any other domain name registrar.

Also, the AWS account you use must have a dedicated public hosted zone in your Route 53 service.

TIP: For details, refer to the Origin Kubernetes Distribution (OKD) documentation: https://docs.openshift.com/container-platform/4.11/installing/installing_aws/installing-aws-account.html#installation-aws-route53_installing-aws-account[Configuring Route 53].

[#setup-external-domain]
==== Configuring an external domain

If you registered the domain name through an external registrar, _not_ Route 53, you need to delegate the domain. To do this, perform these steps:

. Sign in to your AWS account and create a public hosted zone using the *Route 53* service as described in xref:#setup-route-53[]. Use the same domain name as you registered externally.
. In the Route 53 console, go to the public hosted zone you created and check the *`NS`* type record (*name servers* process DNS requests for the domain name). The *Value* column contains a list of NS server names. Save them as they will be needed later.
. Go to the external domain name registrar where you created the domain name.
. Open the domain settings and find the settings related to NS servers.
. Provide the NS servers you copied from the public hosted zone in your AWS account.

==== AWS account limits

The OpenShift Container Platform cluster uses a number of AWS components, and the default _service limits_ affect your ability to install a cluster.

To see a list of AWS components whose limits may impact your ability to install and run an OpenShift Container Platform cluster, refer to the OKD documentation: https://docs.openshift.com/container-platform/4.11/installing/installing_aws/installing-aws-account.html#installation-aws-limits_installing-aws-account[AWS account limits].

NOTE: You must also increase the CPU limit for your Amazon *_on-demand_* virtual machines. For details, refer to the AWS documentation: https://aws.amazon.com/premiumsupport/knowledge-center/ec2-on-demand-instance-vcpu-increase[How do I request an EC2 vCPU limit increase for my On-Demand Instance?]

==== Creating an IAM user

. Before installing OpenShift Container Platform, create an _**IAM** user_. For details, refer to the AWS documentation: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html[Creating an IAM user in your AWS account].
. In addition, perform these important steps:

* Remove any *Service control policies (SCPs*) restrictions from your AWS account.

+
NOTE: When you create a cluster, an associated AWS OpenID Connect (OIDC) identity provider is also created. The OIDC provider configuration is based on the public key stored in the AWS region *`us-east-1`*. Customers using AWS SCP must allow the use of the region *`us-east-1`* even if the cluster is deployed in a different region. If these policies are not configured correctly, permission errors may occur since the OKD installer verifies them.
+
TIP: For details, refer to section *1.1. DEPLOYMENT PREREQUISITES* of the following document: https://access.redhat.com/documentation/en-us/red_hat_openshift_service_on_aws/4/pdf/prepare_your_environment/red_hat_openshift_service_on_aws-4-prepare_your_environment-en-us.pdf[Red Hat OpenShift Service on AWS 4. Prepare your environment].

* Properly configure the *_permissions boundary_* for the IAM user you created.
+
Here is an example of a permissions boundary policy. You can use it or completely remove any permissions boundary.
+
[%collapsible]
._Setting the *permissions boundary_* policy
====
[source,json]
----
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "NotAction": [
                "iam:*"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "iam:Get*",
                "iam:List*",
                "iam:Tag*",
                "iam:Untag*",
                "iam:GenerateServiceLastAccessedDetails",
                "iam:GenerateCredentialReport",
                "iam:SimulateCustomPolicy",
                "iam:SimulatePrincipalPolicy",
                "iam:UploadSSHPublicKey",
                "iam:UpdateServerCertificate",
                "iam:CreateInstanceProfile",
                "iam:CreatePolicy",
                "iam:DeletePolicy",
                "iam:CreatePolicyVersion",
                "iam:DeletePolicyVersion",
                "iam:SetDefaultPolicyVersion",
                "iam:CreateServiceLinkedRole",
                "iam:DeleteServiceLinkedRole",
                "iam:CreateInstanceProfile",
                "iam:AddRoleToInstanceProfile",
                "iam:DeleteInstanceProfile",
                "iam:RemoveRoleFromInstanceProfile",
                "iam:UpdateRole",
                "iam:UpdateRoleDescription",
                "iam:DeleteRole",
                "iam:PassRole",
                "iam:DetachRolePolicy",
                "iam:DeleteRolePolicy",
                "iam:UpdateAssumeRolePolicy",
                "iam:CreateGroup",
                "iam:UpdateGroup",
                "iam:AddUserToGroup",
                "iam:RemoveUserFromGroup",
                "iam:PutGroupPolicy",
                "iam:DetachGroupPolicy",
                "iam:DetachUserPolicy",
                "iam:DeleteGroupPolicy",
                "iam:DeleteGroup",
                "iam:DeleteUserPolicy",
                "iam:AttachUserPolicy",
                "iam:AttachGroupPolicy",
                "iam:PutUserPolicy",
                "iam:DeleteUser",
                "iam:CreateRole",
                "iam:AttachRolePolicy",
                "iam:PutRolePermissionsBoundary",
                "iam:PutRolePolicy"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "iam:CreateAccessKey",
                "iam:DeleteAccessKey",
                "iam:UpdateAccessKey",
                "iam:CreateLoginProfile",
                "iam:DeleteLoginProfile",
                "iam:UpdateLoginProfile",
                "iam:ChangePassword",
                "iam:CreateVirtualMFADevice",
                "iam:EnableMFADevice",
                "iam:ResyncMFADevice",
                "iam:DeleteVirtualMFADevice",
                "iam:DeactivateMFADevice",
                "iam:CreateServiceSpecificCredential",
                "iam:UpdateServiceSpecificCredential",
                "iam:ResetServiceSpecificCredential",
                "iam:DeleteServiceSpecificCredential"
            ],
            "Resource": "*"
        }
    ]
}
----
====

TIP: To learn more about creating an IAM user, refer to the OKD documentation: https://docs.openshift.com/container-platform/4.11/installing/installing_aws/installing-aws-account.html#installation-aws-iam-user_installing-aws-account[Creating an IAM user].

==== Required AWS permissions for the IAM user

To deploy all components of an OpenShift Container Platform cluster, the IAM user requires certain permissions that must be attached to that user.

To see an example of these permissions, refer to the OKD documentation: https://docs.openshift.com/container-platform/4.11/installing/installing_aws/installing-aws-account.html#installation-aws-permissions_installing-aws-account[Required AWS permissions for the IAM user].

[#create-additional-accounts]
=== Creating additional accounts

Before installing OpenShift Container Platform on AWS, you need to create a Docker Hub and Red Hat account.

This is necessary to form a *`docker pull secret`* that will be used later.

==== Creating a Docker Hub account

* Some services use images from Docker Hub repositories. To use them, you need an account. For details, refer to the Docker documentation: https://docs.docker.com/docker-id/[Docker ID accounts].

* Additionally, the limit on the number of images uploaded per day may prevent the services from starting. To avoid this, you'll need to upgrade your subscription to the Pro level. This will change the limit from 200 image pulls per 6 hours to 5,000 image pulls per day. For details, refer to the Docker documentation: https://docs.docker.com/subscription/upgrade/[Upgrade your subscription].

==== Creating a Red Hat account

To download the images required to install OpenShift Container Platform, you need a Red Hat account. For details, refer to the Red Hat documentation: https://access.redhat.com/articles/5832311[Red Hat Login ID and Account].

This is necessary to download the generated pull secret later, as described in xref:#okd-aws-install-preparation[]. It will allow you to authenticate and download container images for OpenShift Container Platform components.

[#deploy-additional-resources-for-okd]
== Deploying additional resources to install an OKD cluster on AWS

Launch the following resources in AWS to install the cluster and platform successfully. The diagram below illustrates the infrastructure scheme with these resources. This is done to simplify the platform installation and avoid unwanted errors associated with setting up from a local computer.

image:installation/aws/installation-aws-1.svg[image,width=468,height=375]

=== Description of additional resources

A more detailed description of the additional resources from the scheme is depicted below:

* *S3 Bucket* is used to store the Terraform state.
* *DynamoDB* is a table in DynamoDB that will be used to store information about the locking of the Terraform state.
* *NAT Gateway* provides a private server with Internet access.
* *Bastion* is an intermediary server that ensures secure and restricted access to the server in a private network. Subsequently, an SSH tunnel will be established through this bastion to the *Initial-node*.
* *Initial-node* is a server in a private network, through which the cluster and platform will be installed.

These resources can be deployed using the prepared Terraform code in the following steps.

==== Recommended Bastion settings

The following table provides the recommended settings for Bastion.

.Bastion settings
[width="100%",cols="6%,33%,61%",options="header",]
|===

|*#* |*Setting* |*Value*

|1 |Instance type |t2.nano
|2 |vCPUs |1
|3 |RAM |0.5 GiB
|4 |CPU Credits/hr |3
|5 |Platform |Ubuntu
|6 |AMI name |ubuntu-bionic-18.04-amd64-server-20210224
|7 |Volume |8 Gb

|===

==== Recommended secondary virtual machine settings

The following table provides the recommended settings for the secondary virtual machine.

.Secondary virtual machine settings
[width="100%",cols="6%,33%,61%",options="header",]
|===

|*#* |*Setting* |*Value*
|1 |Instance type |t2.medium
|2 |vCPUs |2
|3 |RAM |4 GiB
|4 |CPU Credits/hr |24
|5 |Platform |Ubuntu
|6 |AMI name |ubuntu-bionic-18.04-amd64-server-20210224
|7 |Volume |150 Gb

|===

=== Additional settings

==== Installing necessary tools

For further actions, installing the required tools on your local computer is necessary.

* unzip
* https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html[aws cli v2]
* https://docs.docker.com/engine/install/[terraform v1.6.6]

You can check the correct installation of these tools using the following commands:

.Checking the installation of tools
====

.Checking unzip
----
$ unzip -v
----

.Checking aws cli
----
$ aws --version
----

.Checking terraform
----
$ terraform version
----
====

==== Setting up AWS CLI

Authenticate your AWS account using AWS CLI. To do this, execute the following command:

.*AWS account authentication*
[source,bash]
----
$ aws configure
AWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE
AWS Secret Access Key [None]: JalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
Default region name [None]: eu-central-1
Default output format [None]: json
----

TIP: For more detailed information on the AWS account authentication process using AWS CLI, refer to the official AWS documentation: https://docs.aws.amazon.com/cli/latest/userguide/cli-authentication-user.html#cli-authentication-user-configure[Configure the AWS CLI].

==== Setting up AWS cross-account access

Before running the Terraform code, it must be downloaded. This requires access to an AWS S3 bucket where it is stored. This is possible only if a special IAM role is created. Follow these steps to do so:

. Create an AWS IAM role.
+
[source,bash]
----
$ aws iam create-role \
      --role-name UserCrossAccountRole \
      --description "Role for uploading terraform files from AWS S3" \
      --assume-role-policy-document '{
            "Version": "2012-10-17",
            "Statement": [
                {
                    "Action": "sts:AssumeRole",
                    "Effect": "Allow",
                    "Principal": {
                        "AWS": "arn:aws:iam::<YourAccountId>:root"
                    }
                }
            ]
          }'
----
+
[NOTE]
====
*`<YourAccountId>`* — insert your AWS account ID here
====

. Create an AWS IAM policy.
+
[source,bash]
----
$ aws iam create-policy \
      --policy-name UserCrossAccountPolicy \
      --policy-document '{
            "Version": "2012-10-17",
            "Statement": [
                {
                    "Action": "sts:AssumeRole",
                    "Effect": "Allow",
                    "Resource": "arn:aws:iam::764324427262:role/CustomCrossAccountRole"
                }
            ]
          }'
----

. Attach the policy to the role.
+
[source,bash]
----
$ aws iam attach-role-policy \
      --role-name UserCrossAccountRole \
      --policy-arn arn:aws:iam::<YourAccountId>:policy/UserCrossAccountPolicy
----
+
NOTE: *`<YourAccountId>`* -- insert your AWS account ID here.

. Add the required values for the role to the `config` file.
+
[source,bash]
----
$ cat <<EOT >> ~/.aws/config
[profile user-cross-account-role]
role_arn = arn:aws:iam::764324427262:role/CustomCrossAccountRole
source_profile = default
EOT
----

. After this, you must contact the support team, who should add your AWS account ID to the Trust relationships for the `CustomCrossAccountRole` in their AWS account.

==== Downloading Terraform code

. Download the archive with the Terraform code.
+
[source,bash]
----
$ aws s3 cp s3://mdtu-ddm-platform-installer/terraform/terraform.zip terraform.zip  --profile user-cross-account-role
----

. Unzip the Terraform code into a separate directory.
+
[source,bash]
----
$ unzip terraform.zip -d ~/terraform
----

=== Description of the Terraform code

As an example of process automation, Terraform code was implemented, which can be customized to your parameters and used for deploying infrastructure.

==== Initial Terraform code

This Terraform code is designed to create resources for further steps. These resources include:

* S3 Bucket
* DynamoDB Table

{empty}

Initial Code. Description of Terraform files: ::

* `main.tf` – the main configuration Terraform file. It contains modules for creating:
** S3 bucket
** DynamoDB table
* `providers.tf` — used to define the Terraform version, necessary plugins, and AWS provider parameters.
* `variables.tf` — used for describing all the variables used in the Terraform configuration.
* `terraform.tfvars` — contains values for specific variables defined in the Terraform configuration files. If necessary, change the values for the following parameters to the required ones:
** `region` — this variable determines the AWS region in which the resources will be created.
** `tags` — this variable adds tags (labels) to the resources.

==== Main Terraform code

The main Terraform code deploys all the necessary resources. Below is a description of the templates.

Main Code. Description of Terraform files: ::

* `main.tf` — the main configuration Terraform file. It contains modules for creating:
** `VPC`;
** `ec2_bastion`;
** `ec2_instance`;
** `key_pair`.
* `providers.tf` — used to define the Terraform version, necessary plugins, and AWS provider parameters. Be sure to change the values for the following parameters as needed:
** `bucket` — this variable contains the name of the S3 bucket. Change `<ACCOUNT_ID>` to your AWS account ID.
* `iam-node-role.tf` — used to create a special IAM role with the necessary permissions. This will enable setting up AWS cross-account resource access and downloading the Docker image for containers and the Installer.
* `elastic-ip.tf` — used to create an AWS Elastic IP (EIP) resource using Terraform.
* `security-groups.tf` — creates Security Groups, which allow SSH connections (TCP port 22) to the bastion and `initial-node`.
* `ssh-key.tf` — contains code for creating an SSH private key, saving it to a file, and setting its access rights.
* `files/user_data.sh.tpl` is a script template executed when creating or updating an EC2 instance in the AWS environment. This script will do the following for the `initial-node`:
** install Docker;
** install Unzip;
** install AWS CLI v2;
** additionally, set up AWS cross-account resource access.
* `variables.tf` — used for describing all the variables used in the Terraform configuration.
* `terraform.tfvars` — contains values for specific variables defined in the Terraform configuration files. If necessary, change the values for the following parameters as needed:
** `region` — this variable determines the AWS region where resources will be created.
** `platform_name` — this variable adds a name to the cluster and AWS resources.
** `ingress_cidr_blocks` — Add the necessary IP address here to connect via SSH to an additional virtual machine.
** `prefix_list_ids` — if multiple addresses need to be opened for connection, create a `prefix-list` and use its ID in this parameter.
** `tags` — this variable adds tags (labels) to the resources.

=== Running Terraform code

After the changes made in the previous steps, the Terraform code is now ready to be executed.

==== Running the Initial Terraform code

. Sequentially execute the following commands to navigate to the directory with the initial Terraform code and initialize the working Terraform directory.
+
[source,bash]
----
$ cd ~/terraform/initCode

$ terraform init
----


. Use the following command to apply the changes defined in the configuration files and create the resources.

+
[source,bash]
----
$ terraform apply -auto-approve
----

. Wait for the resources to be created.

==== Running the Main Terraform code

. Sequentially execute the following commands to navigate to the directory with the main Terraform code and initialize the working Terraform directory.
+
[source,bash]
----
$ cd ~/terraform/mainCode

$ terraform init
----

. Use the following command to apply the changes defined in the configuration files and create the resources.
+
[source,bash]
----
$ terraform apply -auto-approve
----

. Wait for the resources to be created.

=== Connecting to the additional virtual machine

To connect to the additional virtual machine from your computer, you need to create an SSH tunnel. Use the following command:

.Creating an SSH tunnel
====
----
$ ssh -i <SSH_KEY> -L 1256:<NODE_PRIVATE_IP>:22 -N -f ubuntu@<BASTION_PUBLIC_IP>
----
====

After creating an SSH tunnel, you can connect to the additional virtual machine. Use the following command:

.Connecting via SSH
====
----
$ ssh -i <SSH_KEY> ubuntu@localhost -p 1256
----
====

[IMPORTANT]
====
Additional virtual machine purpose ::
You need to perform all subsequent steps on the additional virtual machine, namely the installation of the cluster and Platform.
====

[#launch-openshift-install]
=== Starting the openshift-install container

To install the cluster using the *`openshift-install`* Docker image, perform the following steps.

. Sign in to AWS Elastic Container Registry (ECR).
+
[source,bash]
----
$ sudo aws ecr get-login-password --profile cross-account-role --region eu-central-1 | docker login --username AWS --password-stdin 764324427262.dkr.ecr.eu-central-1.amazonaws.com
----

. Download the Docker image.
+
[source,bash]
----
$ docker pull 764324427262.dkr.ecr.eu-central-1.amazonaws.com/openshift-install:v3
----

. Tag the Docker image you downloaded.
+
[source,bash]
----
$ docker tag 764324427262.dkr.ecr.eu-central-1.amazonaws.com/openshift-install:v3 openshift-install:v3
----

. Create a new folder to keep all the cluster data.
+
[source,bash]
----
$ mkdir ~/openshift-cluster
----

. Switch to the folder you created.
+
[source,bash]
----
$ cd ~/openshift-cluster
----

. Run the *`openshift-install`* container.
+
[source,bash]
----
$ sudo docker run --rm -it --name openshift-install-v3 \
    --user root:$(id -g) \
    --net host \
    -v $(pwd):/tmp/openshift-cluster \
    --env AWS_ACCESS_KEY_ID=<КЛЮЧ_ДОСТУПУ> \
    --env AWS_SECRET_ACCESS_KEY=<secret-access-key> \
    openshift-install:v3 bash
----

[#okd-aws-install-preparation]
== Preparing to install the OKD cluster on AWS

In OpenShift Container Platform version `4.11`, you can install a customized cluster on infrastructure that the installation program provisions on AWS.

[NOTE]
====
OKD version ::

The recommended OKD version is *`4.11.0-0.okd-2022-08-20-022919`*.
====

To install the cluster, perform the following steps:

. Inside the container, switch to the *_/tmp/openshift-cluster_* folder.
+
[source,bash]
----
$ cd /tmp/openshift-cluster
----

. Follow the steps described in the official documentation on the OKD website, https://docs.openshift.com/container-platform/4.11/installing/installing_aws/installing-aws-customizations.html[Installing a cluster on AWS with customizations], up to the step *Obtaining an AWS Marketplace image*: https://docs.openshift.com/container-platform/4.11/installing/installing_aws/installing-aws-customizations.html#installation-aws-marketplace-subscribe_installing-aws-customizations[Obtaining an AWS Marketplace image].

. Download the customized OKD installer that includes a fix for the blocker issue described at [OCPBUGS-11636](https://issues.redhat.com/browse/OCPBUGS-11636).
+
[source,bash]
----
$ aws s3 cp s3://mdtu-ddm-platform-installer/okd-installer/openshift-install-fix-aws-4.11.0-0.okd-2022-08-20-022-fix-aws.tar.gz openshift-install-fix-aws-4.11.0-0.okd-2022-08-20-022-fix-aws.tar.gz --profile cross-account-role
----

. Unzip the installation program from the downloaded archive.
+
[source,bash]
----
$ tar xvfz openshift-install-fix-aws-4.11.0-0.okd-2022-08-20-022-fix-aws.tar.gz
----
+
CAUTION: To configure the installation, you need to create a *_install-config.yaml_* file and enter the necessary parameters before installing the cluster.

. Create a new directory for the cluster configuration files and the install-config.yaml file. Do this by executing the following commands in sequence:
+
[source,bash]
----
$ mkdir /tmp/openshift-cluster/cluster-state

$ touch /tmp/openshift-cluster/cluster-state/install-config.yaml
----
+
After creating the file, fill it with the necessary parameters. The created configuration file includes only the necessary parameters for a minimal cluster deployment. For customization of settings, refer to the official documentation.
+
Recommended parameters for *_install-config.yaml_*: ::
+
[%collapsible]
.*_install-config.yaml_*
====
[source,yaml]
----
apiVersion: v1
baseDomain: <BASE_DOMAIN>(1)
compute:
  - architecture: amd64
    hyperthreading: Enabled
    name: worker
    platform:
      aws:
        zones:
          - eu-central-1c
        rootVolume:
          size: 80
          type: gp3
        type: r5.2xlarge
        amiID: ami-094fe1584439e91dd
    replicas: 3
controlPlane:
  architecture: amd64
  hyperthreading: Enabled
  name: master
  platform:
    aws:
      zones:
        - eu-central-1c
      rootVolume:
        size: 80
        type: gp3
      type: r5.xlarge
  replicas: 3
metadata:
  name: <CLUSTER_NAME>(2)
networking:
  clusterNetwork:
    - cidr: 10.128.0.0/14
      hostPrefix: 23
  machineNetwork:
    - cidr: 10.0.0.0/16
  networkType: OVNKubernetes
platform:
  aws:
    region: eu-central-1
    userTags:
      'user:tag': <CLUSTER_NAME>(2)
publish: External
pullSecret: <PULL_SECRET>(4)
sshKey: <SSHKEY>(3)
----

* (1) `<BASE_DOMAIN`> -- the domain name you created and configured earlier. For details, see xref:#setup-route-53[] and xref:#setup-external-domain[].

* (2) `<CLUSTER_NAME>` -- the name of the future OKD cluster.

* (3) `<SSHKEY>` -- one or more SSH keys used to access the cluster machines. You can use the same key that was created during the OKD cluster installation, or any other key.
+
TIP: For details, refer to the OKD documentation: https://docs.openshift.com/container-platform/4.11/installing/installing_aws/installing-aws-customizations.html#installation-configuration-parameters-optional_installing-aws-customizations[Optional configuration parameters].

* (4) <PULL_SECRET> -- the secret you created earlier (for details, see xref:#create-additional-accounts[]). You need to get this secret from the Red Hat OpenShift Cluster Manager.
+
TIP: To learn more, refer to step 5 on this OKD page: https://docs.openshift.com/container-platform/4.11/installing/installing_aws/installing-aws-customizations.html#installation-obtaining-installer_installing-aws-customizations[Obtaining the installation program].
+
You need to add your Red Hat and Docker Hub credentials to the pull secret. A combined secret will look as follows:
+
._An example of a combined *pull secret*_
[%collapsible]
=====
[source,json]
----
{
   "auths":{
      "cloud.openshift.com":{
         "auth":"b3Blb=",
         "email":"test@example.com"
      },
      "quay.io":{
         "auth":"b3Blb=",
         "email":"test@example.com"
      },
      "registry.connect.redhat.com":{
         "username":"test",
         "password":"test",
         "auth":"b3Blb=",
         "email":"test@example.com"
      },
      "registry.redhat.io":{
         "username":"test",
         "password":"test",
         "auth":"b3Blb=",
         "email":"test@example.com"
      },
      "index.docker.io/v2/":{
         "username":"test",
         "password":"test",
         "auth":"b3Blb=",
         "email":"test@example.com"
      }
   }
}
----
=====
+
For convenience, the pull secret should be written to the *_install-config.yaml_* file in one line. The final secret will look as follows:
+
._An example of a one-line *pull secret*_
[%collapsible]
=====
----
'{"auths":{"cloud.openshift.com":{"auth":"b3Blb=","email":"test@example.com"},"quay.io":{"auth":"b3Blb=","email":"test@example.com"},"registry.connect.redhat.com":{"username":"test","password":"test","auth":"b3Blb=","email":"test@example.com"},"registry.redhat.io":{"username":"test","password":"test","auth":"b3Blb=","email":"test@example.com"},"index.docker.io/v2/":{"username":"test","password":"test","auth":"b3Blb=","email":"test@example.com"}}}'
----
=====

====
+
WARNING: The Installer deletes the *install-config.yaml* file when creating the cluster. We recommend backing up the *install-config.yaml* file if you need to deploy multiple clusters.

. Also, execute the following command to customize the OpenShift cluster version 4.11 installation. This variable allows you to specify a specific image used during the installation.
+
[source,bash]
----
$ export OPENSHIFT_INSTALL_RELEASE_IMAGE_OVERRIDE="quay.io/openshift/okd:4.11.0-0.okd-2022-08-20-022919"
----

== Running the OKD4 installer and deploying an empty OKD4 cluster

After *_install-config.yaml_* is created, run the following command to deploy the OKD cluster:

.*Installing the OKD cluster*
[source,bash]
----
$ ./openshift-install create cluster --dir /tmp/openshift-cluster/cluster-state --log-level=info
----

NOTE: The cluster deployment process usually takes up to 1 hour.

Upon successful deployment, the following cluster access and credential information displays in your terminal:

* login;
* password;
* a link to the cluster's web console

image:installation/aws/installation-aws-2.png[image,width=468,height=198]

The installation program generates a series of cluster definition files for your cluster in the installation directory, which are necessary for its uninstallation.

TIP: To learn more, refer to the *Prerequisites* section on this OKD page: https://docs.openshift.com/container-platform/4.11/installing/installing_aws/uninstalling-cluster-aws.html#installation-uninstall-clouds_uninstall-cluster-aws[Uninstalling a cluster on AWS].

This directory will also contain the *_/auth_* folder with two authentication files: for working with the cluster through the *web console* and the *OKD command line interface* (OKD CLI).

== Replacing self-signed certificates with trusted certificates

To replace self-signed certificates with trusted ones, you first need to obtain these certificates.

This section describes obtaining free certificates from https://letsencrypt.org/[Let's Encrypt] and installing them on your server.

Let's Encrypt certificates are obtained using the https://github.com/acmesh-official/acme.sh[acme.sh] utility.

TIP: To learn about using Let's Encrypt via the ACME protocol, refer to the Let's Encrypt documentation: https://letsencrypt.org/docs/client-options/[ACME Client Implementations].

To replace the certificates, perform the following steps: ::
+
. Set the environment variable. The variable must point to the *_kubeconfig_* file.
+
[source,bash]
----
$ export KUBECONFIG=cluster-state/auth/kubeconfig
----
. Create the *_letsencrypt.sh_* file and paste the following script into it:
+
._Certificate replacement script_
[%collapsible]
====
[source,bash]
----
#!/bin/bash
yum install -y openssl
mkdir -p certificates
export CERT_HOME=./certificates
export CURDIR=$(pwd)
cd $CERT_HOME

# Clone the acme.sh utility from the GitHub repository
git clone https://github.com/neilpang/acme.sh
sed -i "2i AWS_ACCESS_KEY_ID=\"${AWS_ACCESS_KEY_ID}\"" ./acme.sh/dnsapi/dns_aws.sh
sed -i "3i AWS_SECRET_ACCESS_KEY=\"${AWS_SECRET_ACCESS_KEY}\"" ./acme.sh/dnsapi/dns_aws.sh
cd $CURDIR
# Get API Endpoint URL
export LE_API="$(oc whoami --show-server | cut -f 2 -d ':' | cut -f 3 -d '/' | sed 's/-api././')"
# Get Wildcard Domain
export LE_WILDCARD="$(oc get ingresscontroller default -n openshift-ingress-operator -o jsonpath='{.status.domain}')"
${CERT_HOME}/acme.sh/acme.sh --register-account -m user_${RANDOM}@example.com
${CERT_HOME}/acme.sh/acme.sh --issue -d ${LE_API} -d *.${LE_WILDCARD} --dns dns_aws
export CERTDIR=$CERT_HOME/certificates
mkdir -p ${CERTDIR}

# Transfer certificates from the default acme.sh path to a more convenient directory using the --install-cert - key
${CERT_HOME}/acme.sh/acme.sh --install-cert -d ${LE_API} -d *.${LE_WILDCARD} --cert-file ${CERTDIR}/cert.pem --key-file ${CERTDIR}/key.pem --fullchain-file ${CERTDIR}/fullchain.pem --ca-file ${CERTDIR}/ca.cer
# Create secret
oc create secret tls router-certs --cert=${CERTDIR}/fullchain.pem --key=${CERTDIR}/key.pem -n openshift-ingress
# Update Custom Resource for Router
oc patch ingresscontroller default -n openshift-ingress-operator --type=merge --patch='{"spec": { "defaultCertificate": { "name": "router-certs" }}}'
----
====

. Make the script executable.
+
[source,bash]
----
$ chmod +x ./letsencrypt.sh
----

. Run the script.
+
[source,bash]
----
$ bash -x ./letsencrypt.sh
----

. Exit the container after running the script. To do this, use the following command. The container will be deleted automatically.
+
.Exiting the container
----
$ exit
----

[#installer-preparation-launch]
== Preparing and launching the Installer to deploy and update the Platform on the OKD cluster

Before launching the _Installer_, you need to prepare the workstation where it will run.

=== Deploying from scratch

==== Prerequisites

Before running the Platform installation script, perform the following steps:

. Download the appropriate version of the Installer by running the following sequence of commands.
+
[source,bash]
----
$ mkdir ~/installer

$ cd ~/installer

$ sudo aws s3 cp --profile cross-account-role s3://mdtu-ddm-platform-installer/<VERSION>/mdtu-ddm-platform-<VERSION>.zip mdtu-ddm-platform-<VERSION>.zip
----

. Unpack the Installer to a separate directory.
+
[source,bash]
----
$ unzip mdtu-ddm-platform-(version).zip -d ./installer-<VERSION>
----

. Copy *_kubeconfig_* from the installed cluster.
+
----
$ cp ~/openshift-cluster/cluster-state/auth/kubeconfig ./installer-<VERSION>
----

. Transfer the certificates and `digital-signature-ops` service support files to the *_certificates_* directory and go to the Installer directory.
+
[source,bash]
----
$ cp -r /path/to/folder/certificates/ ./installer-<VERSION>

$ cd installer-<VERSION>
----

==== Configuring MinIO

When deploying the Platform from scratch, no additional configuration is required for MinIO.

==== Configuring Vault

When deploying the Platform from scratch, no additional configuration is required for Vault.

[#deploy-platform-installer-scratch]
==== Deploying the Platform from the Installer

. Run the following commands:
+
[source,bash]
----
$ IMAGE_CHECKSUM=$(sudo docker load -i control-plane-installer.img | sed -r "s#.*sha256:(.*)#\1#" \| tr -d '\n')
----
+
[source,bash]
----
$ echo $IMAGE_CHECKSUM
----
+
[source,bash]
----
$ sudo docker tag ${IMAGE_CHECKSUM} control-plane-installer:<VERSION>
----
. Start the installation process of the new Platform with the images:
+
[source,bash]
----
$ sudo docker run --rm \
    --name control-plane-installer-<VERSION> \
    --user root:$(id -g) \
    --net host \
    -v $(pwd):/tmp/installer \
    --env KUBECONFIG=/tmp/installer/kubeconfig \
    --env idgovuaClientId=f90ab33dc272f047dc330c88e5663b75 \
    --env idgovuaClientSecret=cba49c104faac8c718e6daf3253bc55f2bf11d9e \
    --env deploymentMode=<DEPLOYMENT_MODE> \
    --entrypoint "/bin/sh" control-plane-installer:<VERSION> \
    -c "./install.sh -i"
----
+
[NOTE]
====
* *`--rm`* — this flag will automatically delete the container when it exits. You can remove the flag if you need to inspect the state and logs of the completed container, or if you have an unstable Internet connection.
//* *`DEPLOYMENT_MODE`* -- може бути *`development`* чи *`production`*.
* *`DEPLOYMENT_MODE`* — this variable can be set to *`development`* or *`production`*.
====

==== Deployment status

The following log indicates the Platform update process was successful:

image:admin:installation/aws/installation-aws-3.png[image,width=468,height=178]

If you removed the *`--rm`* flag during the xref:#deploy-platform-installer-scratch[] step, you need to do the following: ::
+
. Run the following command to verify that the container has exited with a status of 0, which indicates that it has completed successfully.
+
[source,bash]
----
$ docker ps --all --latest
----
+
image:admin:installation/aws/installation-aws-4.png[image,width=468,height=26]

. Remove the container using the following command:
+
[source,bash]
----
$ docker rm $(docker ps --latest -q)
----

==== Post-deployment required steps

. After installing the Platform, make sure the *`cluster-management`* pipeline has started and passed successfully (with a green status). [.underline]#_Only after this the Platform will be ready for deploying registries. Without this action, the registries will not deploy_#.
+
You can locate the *`cluster-management`* pipeline using the following path:
+
*_OKD Web UI > control-plane NS > Routes > jenkins url > cluster-mgmt > MASTER-Build-cluster-mgmt_*.

. Request access to the IIT widget as described here: https://eu.iit.com.ua/sign-widget/v20200922/.

[NOTE]
====
Additional resources state ::

After all the steps are completed, you can shut down Bastion and the additional virtual machine. They will not be needed until the next Platform update.
====

=== Updating

==== Prerequisites

Before running the Platform installation script, perform the following steps:

. Download the appropriate version of the Installer by running the following sequence of commands.
+
[source,bash]
----
$ mkdir ~/installer

$ cd ~/installer

$ sudo aws s3 cp --profile cross-account-role s3://mdtu-ddm-platform-installer/<VERSION>/mdtu-ddm-platform-<VERSION>.zip mdtu-ddm-platform-<VERSION>.zip
----

. Unpack the Installer to a separate directory.
+
[source,bash]
----
$ unzip mdtu-ddm-platform-(version).zip -d ./installer-<VERSION>
----

. Copy *_kubeconfig_* from the installed cluster.
+
----
$ cp ~/openshift-cluster/cluster-state/auth/kubeconfig ./installer-<VERSION>
----

. Transfer the certificates and `digital-signature-ops` service support files to the *_certificates_* directory and go to the Installer directory.
+
[source,bash]
----
$ cp -r /path/to/folder/certificates/ ./installer-<VERSION>

$ cd installer-<VERSION>
----

==== Configuring MinIO

. Copy Terraform state data for MinIO from the previous release.
+
[source,bash]
----
$ cp ~/installer/installer-<VERSION>/terraform/minio/aws/terraform.tfstate ./terraform/minio/aws/
----

. Copy the MinIO key from the previous release.
+
[source,bash]
----
$ cp ~/installer/installer-<VERSION>/terraform/minio/aws/private_minio.key ./terraform/minio/aws/
----

[#platform-update-vault]
==== Configuring Vault

. Copy Terraform state data for Vault from the previous release.
+
[source,bash]
----
$ cp ~/installer/installer-<VERSION>/terraform/vault/aws/terraform.tfstate ./terraform/vault/aws/
----

. Copy the Vault key from the previous release.
+
[source,bash]
----
$ ~/installer/installer-<VERSION>/terraform/vault/aws/private.key ./terraform/vault/aws/
----

[#update-platform-installer]
==== Updating the Platform from the Installer

. Run the following commands:
+
[source,bash]
----
$ IMAGE_CHECKSUM=$(sudo docker load -i control-plane-installer.img | sed -r "s#.*sha256:(.*)#\1#" \| tr -d '\n')
----
+
[source,bash]
----
$ echo $IMAGE_CHECKSUM
----
+
[source,bash]
----
$ sudo docker tag ${IMAGE_CHECKSUM} control-plane-installer:<VERSION>
----

. Update the Platform version with the images:
+
[source,bash]
----
$ sudo docker run --rm \
    --name control-plane-installer-<VERSION> \
    --user root:$(id -g) \
    --net host \
    -v $(pwd):/tmp/installer \
    --env KUBECONFIG=/tmp/installer/kubeconfig \
    --env idgovuaClientId=f90ab33dc272f047dc330c88e5663b75 \
    --env idgovuaClientSecret=cba49c104faac8c718e6daf3253bc55f2bf11d9e \
    --env deploymentMode=<DEPLOYMENT_MODE> \
    --entrypoint "/bin/sh" control-plane-installer:<VERSION> \
    -c "./install.sh -u"
----
+
[NOTE]
====
* *`--rm`* — this flag will automatically delete the container when it exits. You can remove the flag if you need to inspect the state and logs of the completed container, or if you have an unstable Internet connection.
* *`DEPLOYMENT_MODE`* — this variable can be set to *`development`* or *`production`*, depending on the previous installation.
====
+
[WARNING]
====
Run the script twice if the generated log does _NOT_ match the point xref:#update-status[].
====

[#update-status]
==== Update status

The following log indicates the Platform update process was successful:

image:admin:installation/aws/installation-aws-3.png[image,width=468,height=178]

If you removed the *`--rm`* flag during the xref:#update-platform-installer[] step, you need to do the following: ::
+
. Run the following command to verify that the container has exited with a status of 0, which indicates that it has completed successfully.
+
[source,bash]
----
$ docker ps --all --latest
----
+
image:admin:installation/aws/installation-aws-4.png[image,width=468,height=26]

. Remove the container using the following command:
+
[source,bash]
----
$ docker rm $(docker ps --latest -q)
----

==== Required steps after update

After updating the Platform from the Installer: ::

. Navigate to the xref:admin:update/overview.adoc[Update] section.
. Perform the necessary specific update steps for your version of the Platform.
. As part of the particular update steps, refresh the xref:update/update_cluster-mgmt.adoc[Platform's infrastructure components] via the Control Plane interface.

[NOTE]
====
Status of additional resources ::

After completing all actions, the bastion and additional virtual machine can be turned off. They will not be needed until the next platform update.
====

== Common errors during the Platform deployment

In this section, we review errors that may occur when deploying the Platform from scratch and provide methods to resolve them.

=== Bootstrap machine error when deploying the OKD cluster

[bootstrap-machine-issue-description]
==== Problem description

The following error occurs during cluster deployment:

.Bootstrap virtual machine error
----
level=error msg=Attempted to gather ClusterOperator status after installation failure: listing ClusterOperator objects: Get "https://api.<CLUSTER_URL>:6443/apis/config.openshift.io/v1/clusteroperators": dial tcp <CLUSTER_IP>:6443: connect: connection refused
level=error msg=Bootstrap failed to complete: Get "https://api.<CLUSTER_URL>:6443/version": dial tcp <CLUSTER_IP>:6443: connect: connection refused
level=error msg=Failed waiting for Kubernetes API. This error usually happens when there is a problem on the bootstrap host that prevents creating a temporary control plane.
----

This error is related to the bootstrap virtual machine and usually happens when there is a problem on the bootstrap host that prevents creating a temporary Control Plane.

[bootstrap-machine-issue-resolving]
==== Solution

. Run the command to remove the cluster, leaving the *`--dir`* parameter the same.
+
.Removing the OKD cluster
----
$ ./openshift-install destroy cluster --dir /tmp/openshift-cluster/cluster-state --log-level info
----

. Wait until the cluster is removed, then run the command to reinstall it.
+
.Reinstalling the cluster
----
$ ./openshift-install create cluster --dir /tmp/openshift-cluster/cluster-state --log-level=info
----

=== Vault token error when deploying the Platform

[vault-token-issue-description]
==== Problem description

When deploying the Platform, during the Vault installation stage, an error may occur where the `vault_root_token` variable returns an empty value:

image:installation/aws/installation-aws-5.png[image,width=468,height=113]

This error can be caused by Vault not starting successfully or skipping some of the Platform installation steps.

[vault-token-issue-resolving]
==== Solution

. Sign in to your AWS account and locate the *`platform-vault-<CLUSTER_NAME>`* virtual machine.

. Connect to the virtual machine using EC2 Instance Connect or SSH.

. Check the Vault status. The *`Initialized`* parameter must be set to `*true*`.
+
.Checking the Vault status
----
$ vault status
----
+
image:installation/aws/installation-aws-6.png[image,width=468,height=182]

. If the status is different, restart Vault.
+
.Restarting Vault
----
$ systemctl restart vault
----

. If this error occurred during the Platform update, check if the Vault key was copied from the previous release as described in xref:#platform-update-vault[].

. Try running the Platform update process again as described in xref:update-platform-installer[].

=== MinIO SSL certificate error when deploying the Platform

[minio-ssl-certificate-issue-description]
==== Problem description

When deploying the Platform, during the MinIO installation stage, the following error may occur:

image:installation/aws/installation-aws-7.png[image,width=468,height=174]

[minio-ssl-certificate-issue-resolving]
==== Solution

. Go to the Installer directory and start the container for Platform installation using the following command:
+
.Running the container
[source,bash]
----
$ cd ~/installer/installer-<VERSION>
$ sudo docker run -it --rm \
    --name control-plane-installer-<VERSION> \
    --user root:$(id -g) \
    --net host \
    -v $(pwd):/tmp/installer \
    --env KUBECONFIG=/tmp/installer/kubeconfig \
    --env idgovuaClientId=f90ab33dc272f047dc330c88e5663b75 \
    --env idgovuaClientSecret=cba49c104faac8c718e6daf3253bc55f2bf11d9e \
    --env deploymentMode=<DEPLOYMENT_MODE> control-plane-installer:<VERSION> bash
----

. Switch to the appropriate directory and set the environment variables.
+
.Setting the environment variables
[source,bash]
----
$ cd /tmp/installer/terraform/minio/aws
$ export AWS_ACCESS_KEY_ID=$(oc get secret/aws-creds -n kube-system -o jsonpath='{.data.aws_access_key_id}' | base64 -d)
$ export AWS_SECRET_ACCESS_KEY=$(oc get secret/aws-creds -n kube-system -o jsonpath='{.data.aws_secret_access_key}' | base64 -d)
$ export CLUSTER_NAME=$(oc get node -l node-role.kubernetes.io/master -o 'jsonpath={.items[0].metadata.annotations.machine\.openshift\.io/machine}' | sed -r 's#.*/(.*)-master.*#\1#')
$ export clusterNameShort="${CLUSTER_NAME::-6}"
$ export baseDomain=$(oc get dns cluster --no-headers -o jsonpath='{.spec.baseDomain}')
$ export route53HostedZone="${baseDomain/${clusterNameShort}./}"
----

. Remove MinIO using Terraform.
+
.Removing MinIO
[source,bash]
----
$ terraform init
$ terraform destroy -var cluster_name="${clusterNameShort}" -var baseDomain="${route53HostedZone}" -auto-approve
----

. Wait until Minio is removed. Exit the container and retry the Platform installation process as described in xref:#deploy-platform-installer-scratch[] if you are deploying the platform from scratch, or xref:#update-platform-installer[], if you are updating the platform.

=== Error sending images to Nexus when deploying the Platform

[send-images-to-nexus-issue-description]
==== Problem description

During Platform deployment, when sending images to Nexus, the following error may occur:

image:installation/aws/installation-aws-8.png[image,width=468,height=228]

This error is related to *skopeo*, a tool that sends images to Nexus. If the image fails to load in 10 minutes, skopeo returns a timeout error.

[send-images-to-nexus-issue-resolving]
==== Solution

Install the Platform from an additional virtual machine as described in xref:#deploy-additional-resources-for-okd[].
