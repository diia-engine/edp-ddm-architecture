= Migrating registries
include::platform:ROOT:partial$templates/document-attributes/default-set-en.adoc[]

include::platform:ROOT:partial$admonitions/language-en.adoc[]

This page provides practical guidance on migration between OKD clusters A and B.

== Notations and abbreviations

* [.underline]#Cluster A#â€”the cluster where the existing registry is deployed.
* [.underline]#Cluster B#â€”the cluster to which the existing registry will be migrated (target cluster).

NOTE: Registry migration is carried out from the latest backup of the existing registry and, according to the instructions, will be transferred from cluster A to cluster B and restored on this cluster.

== Prerequisites for migration

[NOTE]
====
ðŸ“Œ Note on organizing migration::

. _Planning_: It's essential to develop a clear migration schedule. It should include:

* Date and time for backup creation.
* Time for restoration.
* Defined time for service providers to finish their work before the backup.

. _Communication_: Ensure that all service provider users are timely informed:

* Notify users through external communication channels outside the Platform.
* Inform them about completing their work before the scheduled time.

Following these recommendations will ensure a smooth migration process without unnecessary delays and inconvenience for users.
====

. The migration process involves running a bash script that transfers data from clusters A to B. This script must be executed on a Linux platform with an `x86-64` (also known as `AMD64`, Intel 64, or `x64`) microprocessor architecture for successful migration.
. The user transferring the registry to another cluster should be added as an administrator of the Platform on both clusters through *`control-plane-console`*.
+
TIP: See more detailsâ€”xref:admin:registry-management/control-plane-assign-platform-admins.adoc#add-platform-admin-cp[Creating a platform administrator].
. On the cluster to which the registry is being migrated, the platform version should be deployed, where the `control-plane-gerrit` version matches the registry version (for example, platform version -- *`1.9.4.11`*, registry version -- *`1.9.4.7`*, `control-plane-gerrit` version â€“ *`1.9.4.7`*). This version can be verified by a branch in the *`cluster-mgmt`* repository in the central *Gerrit*. If the branch with the registry version exists, then the registry version can be migrated to cluster B. If not, there are two paths:

* Update the Platform on cluster B to match the registry version.
* Update the registry on cluster A to a version already on cluster B.

. Simultaneous access to both cluster A and cluster B.

. Availability of the following commands in the Terminal:

* `oc`
* `velero`
* `rclone`
* `vault`

. Stable internet connection. _The more significant the bandwidth, the faster the migration will be_. Otherwise, a *jumpbox* (with access to both clusters) in AWS or another cloud provider can be used. Using a jumpbox will reduce the time of transferring the backup from one cluster to another.
+
[NOTE]
====
If using a *jumpbox*, verify access to platform Minio/Vault from the *jumpbox* IP address. To get the IP of the *jumpbox*, execute the following command:
----
ssh sshmyip.com
----

Then verify or add the *jumpbox* IP address to the list of allowed CIDRs at the platform management level for both cluster A and cluster B ( _see more on the page xref:admin:registry-management/control-plane-cidr-access-endpoints.adoc[]_).

If there's no access to the control-plane-console, contact the L2 team to check access.
====
+
[IMPORTANT]
====
For registry migration, it's crucial that there are no registry-related resources on cluster B before starting the migration.

_If the registry did not previously exist on this cluster, further actions are unnecessary._

If the registry existed, check/delete the following to remove all resources: ::
* Delete the registry through the Control Plane administrative panel interface.
+
TIP: More details can be found on the page xref:registry-management/control-plane-remove-registry.adoc[].
+
////
Go to the control-plane-console on cluster B (Openshift-console > Projects > control-plane > Networking > control-plane-console), authenticate through openshift-sso, go to the - Registries subdivision, and click on the basket opposite the name of the registry, confirm changes and wait for the registry to be deleted.
////

* Confirm changes and wait for the registry to be deleted.

* After deletion, check for the absence of the project in the central Gerrit component.

** Go to Gerrit (*Openshift* console > *Projects* > *`control-plane`* > *Networking* > *Routes* > *`control-plane-gerrit`* ).
** Authenticate through *openshift-sso*, open the *Browse* menu > *Repositories* and search by the registry name.
** If the search finds the repository, then go to *Openshift*-console > *Projects* > *`control-plane`* > *Home* > *API Explorer* > in the search ( `Filter by kind ...` ) find `gerritproject` > `<registry name>` > *Actions* > *`Delete GerritProject`*.
** After deleting the Gerrit project, go to the Gerrit console and check that the repository is absent. If the repository exists, delete it through the Gerrit console (open the registry repository > *Commands* > *Delete project*).

* Delete the directory in Minio.

** To check the created directories in Minio, go to *MinioUI* (for vSphere clusters this Route can be found in *OpenShift*-console > *Projects* > *`control-plane`* > *Networking* > *Routes* > *`platform-minio-ui`*.

** If there is no Route, go to the secrets at: +
*Openshift*-console > *Project* > *`control-plane`* > *Workloads* > *Secrets* > *`backup-credentials`*, copy the `backup-s3-like-storage-url` field and add the port to the URL (For example, `https://endpoint.com:9001` ).
+
TIP: Authentication data in Minio is located in *Openshift*-console > *Project* > *`control-plane`* > *Secrets* > *`backup-credentials`*, where *`username`* is the field *`backup-s3-like-storage-access-key-id`*, and `*password*` is *`backup-s3-like-storage-secret-access-key`*.

** After authentication, check/delete the directories related to the registry in the bucket. These include:
*** _openshift-backups/backups/<registry-name>*_;
*** _openshift-backups/restic/<registry-name>_;
*** _obc-backups/<registry name>_.

====

== Preparing the registry for migration

[IMPORTANT,caption=Before starting the migration]
Before beginning the migration, it is essential to restrict end-user access to this registry completely.

. Create a backup of the registry on cluster A.
+
Before transferring the registry to a new cluster, initiate the Jenkins process *`Create-registry-backup-<registry name>`*.
+
If the Jenkins pipeline completes with a *`Success`* status, the backup has been successfully created.
+
[NOTE]
====
To obtain the backup name, go to the logs/event journal of the last Jenkins pipeline run (*Console Output*) and search the page for a message like:

----
[INFO] Velero backup - <registry name>-<timestamp> done with Completed status
----

For example:

----
[INFO] Velero backup - abc-02-2023-04-18-19-03-14 done with Completed status
----

* where *`abc-02-2023-04-18-19-03-14`* is the backup name.

====
+
[WARNING]
====
For registry versions < 1.9.3, execute the following command in Terminal:

----
velero backup describe <backup name>
----

Find the backup name in the logs of the last run of the Jenkins process *`Create-registry-backup-<registry name>`*.
====
+
[TIP]
====
For more information on creating backups and restoring registries, see xref:backup-restore/overview.adoc[].
====

. You can proceed if the last velero backup is finished with a *`Completed`* status. If the velero backup status is different from `Completed`, involve L2-L3 technical support specialists to check the operability of the Jenkins pipeline.

. Obtain consistent data in backups of the migrating registry buckets.
+
Get the latest backups of S3 buckets in the `velero` project. Open the *Workloads* section, then go to *CronJobs*. Use the search panel here to filter buckets by the registry name, for example, `migrationreg`.
+
.CronJobs
image::admin:migrate-registry/migrate-registry-01.png[]

.. Open each *CronJob* and change its start time to the nearest possible. For example, set it to run in the next 10-15 minutes. To do this, go to the settings of each CronJob, open its *YAML* configuration, and change the `spec.schedule` parameter. For instance, to run a CronJob daily at 10:50 UTC, use the following configuration:
+
.CronJob details. YAML Configuration
[source,yaml]
----
spec:
  schedule: 50 10 * * *
----
+
[CAUTION]
====
When working with `cron`, the time is set in https://time.is/UTC[UTC].
====
+
.CronJob details. Schedule
image::admin:migrate-registry/migrate-registry-02.png[]

.. After this, wait for all the CronJobs to start and complete. You can check the progress and status in the *Jobs* section by selecting the relevant Job and viewing the *Status* section, where a `âœ… Complete` mark should be present.
+
.CronJob details. Jobs
image::admin:migrate-registry/migrate-registry-03.png[]
+
.Job details. Status
image::admin:migrate-registry/migrate-registry-04.png[]

.. By following these steps, you will obtain consistent data from the backups of registry buckets in the migration process.

. Prohibit changes to the registry through Jenkins pipelines.
+
In each pipeline for the registry, go to the *Configure* section and find the *`Disable this project`* option under the *Build Triggers* section, check the box next to it, and save the changes using the *`Save`* button.

== Migrating the backup from cluster A to cluster B

. Obtain the login commands for both clusters.
+
For this, log in to the Openshift console and in the top right corner, by clicking on your username, go to *`Copy login command`*, copy the access token in the *`Log in with token`* field, and save it in a text editor.

+
NOTE: Repeat this operation for both clusters: A and B.

. Obtain the name of the latest backup created on cluster A (for example, `abc-02-2023-04-18-19-03-14`).

. Open the terminal and execute the following commands:
+
.Export login for Cluster A
----
export A_CLUSTER_LOGIN="oc login --token â€¦"
----
+
Insert between the quotes *`"..."`* after `--token` the login command obtained in step 1 for Cluster A. There should be no line break at the end of the login command.

+
.Export login for Cluster B
----
export B_CLUSTER_LOGIN="oc login --token â€¦"
----
+
Insert between the quotes *`"..."`* after `--token` the login command obtained in step 1 for Cluster B. There should be no line break at the end of the login command.

+
.Export Registry Name
----
export REGISTRY_NAME="abc-02"
----
+
TIP: `abc-02` is the registry name

+
.Export Backup Name
----
export BACKUP_NAME="<backup name>"
----
+
TIP: Example of a backup name: `*abc-02-2023-04-18-19-03-14*`.
+
[WARNING]
====
If the registry was previously migrated to Cluster A rather than deployed on this Platform, execute an additional *`export`*:

[source,bash]
----
export VAULT_KEY="<key name>"
----

* where *`<key name>`* is the key for the unseal process, which can be found in the *Openshift* console (Cluster A) > *Projects* > `<registry name>` > *ConfigMaps* > *`hashicorp-vault-config`*. The field *key_name* is the key name.
+
For example:
+
[source,hcl]
----
key_name        = "autounseal-migration"
----

====
+
[WARNING]
====
In the case of migrating an extensive registry, perform the export of the following variable:
[source,bash]
----
export LARGE_DATA="true"
----
====
. Download and unzip the archive with the command:
+
----
unzip registry-migration.zip -d registry-migration
----
+
Navigate to the registry-migration directory (`cd`) and execute the command:
+
----
chmod +x migration.sh && ./migration.sh
----

. After executing the script, log in to the terminal using *oc cli* on Cluster B and verify the following:

* The presence of velero backup on Cluster B.
* The presence of directories named _keycloak-export_ in the folder where the script is located.

== Preparing for restoration on cluster B

. Create the registry through *`control-plane-console`*.

* Create a registry with the same name and version on Cluster B. When creating the registry, assign all administrators who were in the registry on Cluster A and provide up-to-date data.
+
[NOTE]
====
Registry key data ::
Fill in the fields either with the current keys for this registry or use test keys. After the migration, the key data can be updated through the *Control Plane* console. For key data, refer to L2-L3 support.
+
For more information on updating registry keys, see xref:admin:registry-management/system-keys/control-plane-registry-keys.adoc[].

Registry template ::
Choose the same template as the one used for this registry on Cluster A. To obtain the template name, go to the *Openshift* console > *Projects* > *`control-plane`* > *API Explorer* > In the search define `codebase` > Go to `codebase` > *Instances* > Open `codebase <registry name>` > Check the following settings:
+
.codebase.yaml
=====
----
metadata:
  annotations:
    registry-parameters/template-name: templates/registry-tenant-template-minimal
----
* where *`templates/registry-tenant-template-minimal`* is the registry deployment template name.
=====
====
+
NOTE: If the console functionality allows adding DNS for Keycloak or portals, skip this step, as traffic is still configured to Cluster A).

* After creation, immediately go to Jenkins (namespace *`control-plane`* > *Networking* > *Routes* > *`jenkins`*), and stop the first build *`MASTER-Build-<registry name>`*.
+
NOTE: Wait for the creation of the directory `<registry name>` and the Jenkins pipeline. After starting, immediately do an *Abort* of the build.

. While still in the Jenkins console, change the configuration of *MASTER-Build-`<registry name>`*: +
Go to *MASTER-Build-`<registry name>`* > *Configure*, and in the *Build Triggers* section, check the box for *Disable this project*. Then save the changes with the *`Save`* button.

. Transfer the *_values.yaml_* and *_values.gotmpl_* configuration files from the registry repository of Cluster A to Cluster B.

* Go to the registry repository on Cluster A: +
Open *Control-plane-console* > *Dashboard* > *Gerrit* > *Browse* > *Repositories* > select the repository *`<registry name>`*. +
In the registry repository, go to *Branches* > `master`, then to *deploy-templates* and open the *_values.yaml_* ( *_values.gotmpl_* ) file > Copy the *raw* code to the clipboard.
* Then go to the registry repository on Cluster B: +
*Control-plane-console* > *Dashboard* > *Gerrit* ) > *Browse* > *Repositories* and select the repository *`<registry name>`*. Through *commands* > *`Create change`*, create a change with the following parameters:

** `Select branch for new change: master`.
** `Description: Update registry before migration`.
+
After creating the change, in the change itself click *`Edit`* > *`ADD/OPEN/UPLOAD`* -- find the *_values.yaml_* (*_values.gotmpl_*) file.
Transfer the copied *_values.yaml_* (*_values.gotmpl_*) configuration from Cluster A to this file.
* Repeat the operation for both files: *_values.yaml_* and *_values.gotmpl_*.
* Save the changes, wait for the *Code Review* pipeline to pass (*CI Jenkins `+1`*), set `*Code-review +2*`, and merge the changes to the `master` branch with the `*Submit*` button.

. Go to Jenkins, change the configuration of *MASTER-Build-`<registry name>`*: +
Go to *MASTER-Build-`<registry name>`* > *Configure*, and in the *Build Triggers* section, uncheck the box for *Disable this project*. Then save the changes with the *`Save`* button.

== Restoring the registry on cluster B

IMPORTANT: Enable access for end users to the registry _ONLY_ after the registry restoration process is complete.

. Open Jenkins (namespace *`control-plane`* > *Networking* > *Routes* > *`jenkins`*), navigate to the folder with the registry name and start the Jenkins pipeline *`Restore-registry-<registry name>`*. After launching the pipeline, select the version (at the `cleanup-registry-before-restore` stage) and wait for the process to complete.
+
NOTE: The process may finish with a *`UNSTABLE`* status. This is related to the configuration change of the pipeline *MASTER-Build-`<registry-name>`*, where `<registry-name>` is the name of the registry.
+
NOTE: If the process finishes with an error or takes more than 1-2 hours, contact L2-L3 technical support specialists.

. After the pipeline completes, go to the Openshift console -> Projects -> `<registry-name>`, and ensure that there are no pods in error states and all pods are in `Running/Completed` status.
+
[NOTE]
====
If a pod named *`bpms-*`* is not started and has an error status, correct the passwords in `postgres` for *`operational-instance`* and *`analytical-instance`* pods. For this, you need to:

* Go to *Openshift* console > *Secrets*, find the secret for `operational-instance` -- *`operational-pguser-postgres`* (for `analytical-instance` -- it is *`analytical-pguser-postgres`*).
* Go to *Secret* and copy the *`password`* field.
* Go to *Openshift* console > *Pods* > find the *`operational-instance`* or *`analytical-instance`* pod and execute the following commands sequentially:
+
[source,bash]
----
psql
----
+
[source,sql]
----
ALTER ROLE postgres WITH PASSWORD '<password>';
----

** where *`<password>`* is the `password` field, copied from *Secret*, for the respective instance -- `operational` or `analytical`.

* After performing all operations, delete the *`bpms`* pod and wait until it is in *`Running`* status (active/started).
====
+
[NOTE]
====
If the *`registry-rest-api`* pod starts with an `ImagePullBackOff` error, add the IP of Cluster B to the annotation of *Openshift Route* > *Nexus*.

* For this, go to *Openshift* console > *Project* > `<registry name>` > *Routes* > *Nexus* > *YAML* and check the following field in the _.yaml_ configuration:
+
.route.yaml
=====
----
metadata:
  annotations:
    haproxy.router.openshift.io/ip_whitelist: <NAT Cluster IP>/32,....
----
=====
+
If the IP address of Cluster B is missing, add it to *`haproxy.router.openshift.io/ip_whitelist`* with a mask of *`/32`*.

====
+
. Go to Jenkins and change the configuration of *`MASTER-Build-<registry-name>`*:
+
Go to *`MASTER-Build-<registry-name>`* > *`Configure`*, and in the *`BuildTriggers`* section, uncheck the box for *Disable this project*. Then save the changes with the *`Save`* button.

+
. Transfer the registry configuration to *_values.yaml/values.gotmpl_*.
+
* Log in to *_control-plane-gerrit_* (*Openshift* console > *Projects* -> *`control-plane`* -> *Networking* -> *`gerrit`* > Log in via *`openshift-sso`*).
+
Go to *Browse* > *Repositories* in Gerrit and select the repository *`<registry name>`*. Through *`commands`* > *`Create change`*, create a change with the following parameters:

** `Select branch for new change: master`.
** `Description: Update registry after migration`.
+
After creating the change, click *`Edit`* in the change itself.

* Add `vault` configuration to *_values.gotmpl_*.
+
For this, take the current `vault` configuration from the config-map *`hashicorp-vault-config`* (*Openshift* console > *Projects* > `<registry name>` > *Workloads* > *ConfigMaps* > *`hashicorp-vault-config`*) and copy the field as in the following example:
+
----
ui = true

listener "tcp" {
  tls_disable = 1
  address = "[::]:8200"
  cluster_address = "[::]:820

1"
}
storage "file" {
  path = "/vault/data"
}
seal "transit" {
   address         = "https://<vault url>"
   disable_renewal = "false"
   key_name        = "<key name>"
   mount_path      = "transit/"
   tls_skip_verify = "true"
}
----
+
* where *`<vault URL>`* is the link to *`vault`*, *`<key name>`* is the key name (the config-map will have the current fields).
+
Then, in the change, click *`ADD/OPEN/UPLOAD`*; in the search, specify *_values.gotmpl_* and select the required file. In the file itself, add the configuration as in the example:
+
[source,yaml]
----
vault:
  platformVaultToken: {{ env "platformVaultToken" }}
  openshiftApiUrl: {{ env "openshiftApiUrl" }}
  centralVaultUrl: {{ b64dec $centralVaultUrl }}
  server:
    dataStorage:
      storageClass: ocs-storagecluster-ceph-rbd
    auditStorage:
      storageClass: ocs-storagecluster-ceph-rbd

    standalone:
      config: |
       ui = true

       listener "tcp" {
         tls_disable = 1
         address = "[::]:8200"
         cluster_address = "[::]:8201"
       }
       storage "file" {
         path = "/vault/data"
       }
       seal "transit" {
          address         = "https://<vault url>"
          disable_renewal = "false"
          key_name        = "<key name>"
          mount_path      = "transit/"
          tls_skip_verify = "true"
       }
----

* After adding, click *Save and Publish*. Wait for the Jenkins Code-Review pipeline to pass (a +1 rating from edp-ci should appear), then click *Reply* > *Code-Review - +2* > *Submit*

. After applying the changes, the Jenkins process *`MASTER-Build-<registry name>`* should start. Wait for the Jenkins pipeline *`MASTER-Build-<registry name>`* to complete.

. Transfer users.
+
In the Keycloak admin console, go to the realm (1), and in the left menu of the realm, select *`Import`* (2). For the import, choose the *`SKIP`* strategy, then click *`Select file`* (3) and select the file from the directory _keycloak-export/<realm name>-users-*.json.
+
NOTE: If there is more than one file, import all files.

+
image:admin:migrate-registry/migrate-registry-2.png[image,width=601,height=417]

. Correct Jenkins Credentials in the registry Jenkins.
+
[NOTE]
====
If you do not have access, add yourself as a registry administrator through control-plane-console.
====
* For this, go to *Openshift console* > *Projects* > `<registry name>` > *Workloads* > *Secrets* > *`gerrit-control-plane-sshkey`* and copy the *`id_rsa`* field.

* Then go to the registry Jenkins (*Networking* > *Routes* > `*jenkins*`) > Manage Jenkins > Manage Credentials > *`edp-ci`* (*`gerrit-control-plane-sshkey`*) > click *`Update`*.

* In the *`Private Key`* field, insert the copied value using *`Replace`*.

. After updating the *`Private Key`*, go to the folder *`registry-regulations`* and select the pipeline *`MASTER-Build-registry-regulations`*, then click *`Build with Parameters`* and select the checkbox for *`FULL_DEPLOY`* and click *`Build`*. Wait for it to complete with a *`Success`* status.

== Testing the registry

. Ensure that the User portals function as expected and that business processes have migrated successfully.

. All Jenkins pipelines should complete with a *`Success`* status.

== Migrating registry configuration

Transfer the registry configuration from Cluster A to Cluster B according to the documentation: ::

* *Administrators* (_see more on the page xref:registry-develop:registry-admin/create-users/create-registry-admins.adoc[]_).
* *Key data*  (_see more on the page xref:admin:registry-management/system-keys/control-plane-registry-keys.adoc[]_).
* *Mail server* (_see more on the page xref:registry-develop:registry-admin/user-notifications/email/config-smtp-server.adoc[]_).
* *Registry resources*
+
[NOTE]
Transfer the settings parameters from the _values.yaml_ file (section `global.registry` ) of the registry on Cluster A to the settings in the _values.yaml_ file of the registry on Cluster B.

* DNS (_see more on the page xref:admin:registry-management/custom-dns/custom-dns-overview.adoc[]_).
* *Access restrictions* (_see more on the page xref:admin:registry-management/control-plane-cidr-access-endpoints.adoc[]_).
* *Service provider authentication* (_see more on the pages xref:registry-develop:registry-admin/cp-auth-setup/cp-auth-setup-officers.adoc[] and xref:registry-develop:registry-admin/cp-auth-setup/cp-officer-self-registration.adoc[]_).
* *Service recipient authentication* (_see more on the page xref:registry-develop:registry-admin/cp-auth-setup/cp-auth-setup-citizens.adoc[]_)
* *Backup* (_see more on the pages xref:admin:backup-restore/control-plane-backup-restore.adoc[] and xref:admin:backup-restore/backup-schedule-registry-components.adoc[]_).